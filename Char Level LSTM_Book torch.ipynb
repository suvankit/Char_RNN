{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adaf0734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2e04a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "#text file opened and read in data as `text`\n",
    "with open('C:/Users/subha/OneDrive/Desktop/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb98e079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictonaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "\n",
    "chars = tuple(set(text))\n",
    "\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ef6f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c797d77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 44, 66, 51, 55, 15, 40, 58,  5, 74, 74, 74,  1, 66, 51, 51, 11,\n",
       "       58, 16, 66, 29, 42, 59, 42, 15, 76, 58, 66, 40, 15, 58, 66, 59, 59,\n",
       "       58, 66, 59, 42, 38, 15, 47, 58, 15, 79, 15, 40, 11, 58, 18, 39, 44,\n",
       "       66, 51, 51, 11, 58, 16, 66, 29, 42, 59, 11, 58, 42, 76, 58, 18, 39,\n",
       "       44, 66, 51, 51, 11, 58, 42, 39, 58, 42, 55, 76, 58, 24, 35, 39, 74,\n",
       "       35, 66, 11, 14, 74, 74, 61, 79, 15, 40, 11, 55, 44, 42, 39])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ddcc02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d8f41d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        \n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        \n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b463c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dedf5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[ 2 44 66 51 55 15 40 58  5 74]\n",
      " [58 66 29 58 39 24 55 58 33 24]\n",
      " [79 42 39 14 74 74 41 52 15 76]\n",
      " [39 58 17 18 40 42 39 33 58 44]\n",
      " [58 42 55 58 42 76 80 58 76 42]\n",
      " [58 78 55 58 35 66 76 74 24 39]\n",
      " [44 15 39 58 54 24 29 15 58 16]\n",
      " [47 58 32 18 55 58 39 24 35 58]\n",
      " [55 58 42 76 39 81 55 14 58 72]\n",
      " [58 76 66 42 17 58 55 24 58 44]]\n",
      "\n",
      "y\n",
      " [[44 66 51 55 15 40 58  5 74 74]\n",
      " [66 29 58 39 24 55 58 33 24 42]\n",
      " [42 39 14 74 74 41 52 15 76 80]\n",
      " [58 17 18 40 42 39 33 58 44 42]\n",
      " [42 55 58 42 76 80 58 76 42 40]\n",
      " [78 55 58 35 66 76 74 24 39 59]\n",
      " [15 39 58 54 24 29 15 58 16 24]\n",
      " [58 32 18 55 58 39 24 35 58 76]\n",
      " [58 42 76 39 81 55 14 58 72 44]\n",
      " [76 66 42 17 58 55 24 58 44 15]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c2a941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## Define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## Define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## Define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hc`. '''\n",
    "        \n",
    "        ## Get x, and the new hidden state (h, c) from the lstm\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        \n",
    "        ## Ppass x through the dropout layer\n",
    "        x = self.dropout(x)\n",
    "        #x.is_contiguous()==True\n",
    "        # Stack up LSTM outputs using view\n",
    "        x = x.contiguous().view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        ## Put x through the fully-connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Return x and the hidden state (h, c)\n",
    "        return x, (h, c)\n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        \n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        \n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "018a0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        h = net.init_hidden(n_seqs)\n",
    "        \n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            \n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                \n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    \n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50bac6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "839b33e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize and print the network\n",
    "net = CharRNN(chars, n_hidden=512, n_layers=2)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f06f4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25... Step: 10... Loss: 3.3370... Val Loss: 3.3316\n",
      "Epoch: 1/25... Step: 20... Loss: 3.1898... Val Loss: 3.2156\n",
      "Epoch: 1/25... Step: 30... Loss: 3.1037... Val Loss: 3.0916\n",
      "Epoch: 1/25... Step: 40... Loss: 2.9192... Val Loss: 2.9271\n",
      "Epoch: 1/25... Step: 50... Loss: 2.7824... Val Loss: 2.7507\n",
      "Epoch: 1/25... Step: 60... Loss: 2.6235... Val Loss: 2.6460\n",
      "Epoch: 1/25... Step: 70... Loss: 2.5546... Val Loss: 2.5676\n",
      "Epoch: 1/25... Step: 80... Loss: 2.4853... Val Loss: 2.5111\n",
      "Epoch: 1/25... Step: 90... Loss: 2.4574... Val Loss: 2.4631\n",
      "Epoch: 1/25... Step: 100... Loss: 2.3917... Val Loss: 2.4250\n",
      "Epoch: 1/25... Step: 110... Loss: 2.3516... Val Loss: 2.3943\n",
      "Epoch: 1/25... Step: 120... Loss: 2.2955... Val Loss: 2.3667\n",
      "Epoch: 1/25... Step: 130... Loss: 2.3094... Val Loss: 2.3394\n",
      "Epoch: 2/25... Step: 140... Loss: 2.2801... Val Loss: 2.3133\n",
      "Epoch: 2/25... Step: 150... Loss: 2.2478... Val Loss: 2.3146\n",
      "Epoch: 2/25... Step: 160... Loss: 2.2364... Val Loss: 2.2684\n",
      "Epoch: 2/25... Step: 170... Loss: 2.2042... Val Loss: 2.2418\n",
      "Epoch: 2/25... Step: 180... Loss: 2.1555... Val Loss: 2.2167\n",
      "Epoch: 2/25... Step: 190... Loss: 2.0972... Val Loss: 2.2029\n",
      "Epoch: 2/25... Step: 200... Loss: 2.1132... Val Loss: 2.1746\n",
      "Epoch: 2/25... Step: 210... Loss: 2.0871... Val Loss: 2.1596\n",
      "Epoch: 2/25... Step: 220... Loss: 2.0585... Val Loss: 2.1442\n",
      "Epoch: 2/25... Step: 230... Loss: 2.0726... Val Loss: 2.1270\n",
      "Epoch: 2/25... Step: 240... Loss: 2.0437... Val Loss: 2.1113\n",
      "Epoch: 2/25... Step: 250... Loss: 2.0033... Val Loss: 2.0933\n",
      "Epoch: 2/25... Step: 260... Loss: 1.9625... Val Loss: 2.0806\n",
      "Epoch: 2/25... Step: 270... Loss: 1.9946... Val Loss: 2.0580\n",
      "Epoch: 3/25... Step: 280... Loss: 1.9865... Val Loss: 2.0488\n",
      "Epoch: 3/25... Step: 290... Loss: 1.9747... Val Loss: 2.0419\n",
      "Epoch: 3/25... Step: 300... Loss: 1.9392... Val Loss: 2.0137\n",
      "Epoch: 3/25... Step: 310... Loss: 1.9220... Val Loss: 2.0073\n",
      "Epoch: 3/25... Step: 320... Loss: 1.8917... Val Loss: 2.0000\n",
      "Epoch: 3/25... Step: 330... Loss: 1.8801... Val Loss: 1.9970\n",
      "Epoch: 3/25... Step: 340... Loss: 1.9279... Val Loss: 1.9696\n",
      "Epoch: 3/25... Step: 350... Loss: 1.8884... Val Loss: 1.9542\n",
      "Epoch: 3/25... Step: 360... Loss: 1.8155... Val Loss: 1.9467\n",
      "Epoch: 3/25... Step: 370... Loss: 1.8608... Val Loss: 1.9382\n",
      "Epoch: 3/25... Step: 380... Loss: 1.8534... Val Loss: 1.9288\n",
      "Epoch: 3/25... Step: 390... Loss: 1.8229... Val Loss: 1.9181\n",
      "Epoch: 3/25... Step: 400... Loss: 1.8023... Val Loss: 1.9084\n",
      "Epoch: 3/25... Step: 410... Loss: 1.8115... Val Loss: 1.8959\n",
      "Epoch: 4/25... Step: 420... Loss: 1.8010... Val Loss: 1.8913\n",
      "Epoch: 4/25... Step: 430... Loss: 1.8095... Val Loss: 1.8770\n",
      "Epoch: 4/25... Step: 440... Loss: 1.7882... Val Loss: 1.8653\n",
      "Epoch: 4/25... Step: 450... Loss: 1.7385... Val Loss: 1.8582\n",
      "Epoch: 4/25... Step: 460... Loss: 1.7207... Val Loss: 1.8551\n",
      "Epoch: 4/25... Step: 470... Loss: 1.7708... Val Loss: 1.8465\n",
      "Epoch: 4/25... Step: 480... Loss: 1.7385... Val Loss: 1.8354\n",
      "Epoch: 4/25... Step: 490... Loss: 1.7540... Val Loss: 1.8310\n",
      "Epoch: 4/25... Step: 500... Loss: 1.7523... Val Loss: 1.8218\n",
      "Epoch: 4/25... Step: 510... Loss: 1.7188... Val Loss: 1.8196\n",
      "Epoch: 4/25... Step: 520... Loss: 1.7303... Val Loss: 1.8100\n",
      "Epoch: 4/25... Step: 530... Loss: 1.7016... Val Loss: 1.8014\n",
      "Epoch: 4/25... Step: 540... Loss: 1.6730... Val Loss: 1.7942\n",
      "Epoch: 4/25... Step: 550... Loss: 1.7150... Val Loss: 1.7906\n",
      "Epoch: 5/25... Step: 560... Loss: 1.6815... Val Loss: 1.7830\n",
      "Epoch: 5/25... Step: 570... Loss: 1.6670... Val Loss: 1.7707\n",
      "Epoch: 5/25... Step: 580... Loss: 1.6543... Val Loss: 1.7922\n",
      "Epoch: 5/25... Step: 590... Loss: 1.6622... Val Loss: 1.7685\n",
      "Epoch: 5/25... Step: 600... Loss: 1.6534... Val Loss: 1.7579\n",
      "Epoch: 5/25... Step: 610... Loss: 1.6266... Val Loss: 1.7478\n",
      "Epoch: 5/25... Step: 620... Loss: 1.6342... Val Loss: 1.7433\n",
      "Epoch: 5/25... Step: 630... Loss: 1.6580... Val Loss: 1.7388\n",
      "Epoch: 5/25... Step: 640... Loss: 1.6233... Val Loss: 1.7295\n",
      "Epoch: 5/25... Step: 650... Loss: 1.6115... Val Loss: 1.7285\n",
      "Epoch: 5/25... Step: 660... Loss: 1.5970... Val Loss: 1.7241\n",
      "Epoch: 5/25... Step: 670... Loss: 1.6126... Val Loss: 1.7232\n",
      "Epoch: 5/25... Step: 680... Loss: 1.6129... Val Loss: 1.7154\n",
      "Epoch: 5/25... Step: 690... Loss: 1.5904... Val Loss: 1.7128\n",
      "Epoch: 6/25... Step: 700... Loss: 1.5913... Val Loss: 1.7057\n",
      "Epoch: 6/25... Step: 710... Loss: 1.5740... Val Loss: 1.6972\n",
      "Epoch: 6/25... Step: 720... Loss: 1.5671... Val Loss: 1.6985\n",
      "Epoch: 6/25... Step: 730... Loss: 1.5885... Val Loss: 1.6912\n",
      "Epoch: 6/25... Step: 740... Loss: 1.5516... Val Loss: 1.6893\n",
      "Epoch: 6/25... Step: 750... Loss: 1.5311... Val Loss: 1.6769\n",
      "Epoch: 6/25... Step: 760... Loss: 1.5832... Val Loss: 1.6756\n",
      "Epoch: 6/25... Step: 770... Loss: 1.5552... Val Loss: 1.6681\n",
      "Epoch: 6/25... Step: 780... Loss: 1.5406... Val Loss: 1.6646\n",
      "Epoch: 6/25... Step: 790... Loss: 1.5294... Val Loss: 1.6680\n",
      "Epoch: 6/25... Step: 800... Loss: 1.5439... Val Loss: 1.6604\n",
      "Epoch: 6/25... Step: 810... Loss: 1.5197... Val Loss: 1.6645\n",
      "Epoch: 6/25... Step: 820... Loss: 1.4923... Val Loss: 1.6544\n",
      "Epoch: 6/25... Step: 830... Loss: 1.5374... Val Loss: 1.6490\n",
      "Epoch: 7/25... Step: 840... Loss: 1.5033... Val Loss: 1.6524\n",
      "Epoch: 7/25... Step: 850... Loss: 1.5137... Val Loss: 1.6439\n",
      "Epoch: 7/25... Step: 860... Loss: 1.4925... Val Loss: 1.6373\n",
      "Epoch: 7/25... Step: 870... Loss: 1.5049... Val Loss: 1.6298\n",
      "Epoch: 7/25... Step: 880... Loss: 1.5008... Val Loss: 1.6290\n",
      "Epoch: 7/25... Step: 890... Loss: 1.5078... Val Loss: 1.6235\n",
      "Epoch: 7/25... Step: 900... Loss: 1.4976... Val Loss: 1.6210\n",
      "Epoch: 7/25... Step: 910... Loss: 1.4513... Val Loss: 1.6211\n",
      "Epoch: 7/25... Step: 920... Loss: 1.4818... Val Loss: 1.6235\n",
      "Epoch: 7/25... Step: 930... Loss: 1.4758... Val Loss: 1.6106\n",
      "Epoch: 7/25... Step: 940... Loss: 1.4711... Val Loss: 1.6120\n",
      "Epoch: 7/25... Step: 950... Loss: 1.4921... Val Loss: 1.6084\n",
      "Epoch: 7/25... Step: 960... Loss: 1.4933... Val Loss: 1.6038\n",
      "Epoch: 7/25... Step: 970... Loss: 1.5023... Val Loss: 1.6127\n",
      "Epoch: 8/25... Step: 980... Loss: 1.4517... Val Loss: 1.6058\n",
      "Epoch: 8/25... Step: 990... Loss: 1.4621... Val Loss: 1.5972\n",
      "Epoch: 8/25... Step: 1000... Loss: 1.4478... Val Loss: 1.5930\n",
      "Epoch: 8/25... Step: 1010... Loss: 1.4863... Val Loss: 1.5848\n",
      "Epoch: 8/25... Step: 1020... Loss: 1.4671... Val Loss: 1.5920\n",
      "Epoch: 8/25... Step: 1030... Loss: 1.4543... Val Loss: 1.5835\n",
      "Epoch: 8/25... Step: 1040... Loss: 1.4500... Val Loss: 1.5887\n",
      "Epoch: 8/25... Step: 1050... Loss: 1.4375... Val Loss: 1.5859\n",
      "Epoch: 8/25... Step: 1060... Loss: 1.4327... Val Loss: 1.5819\n",
      "Epoch: 8/25... Step: 1070... Loss: 1.4505... Val Loss: 1.5752\n",
      "Epoch: 8/25... Step: 1080... Loss: 1.4503... Val Loss: 1.5745\n",
      "Epoch: 8/25... Step: 1090... Loss: 1.4199... Val Loss: 1.5725\n",
      "Epoch: 8/25... Step: 1100... Loss: 1.4145... Val Loss: 1.5729\n",
      "Epoch: 8/25... Step: 1110... Loss: 1.4268... Val Loss: 1.5782\n",
      "Epoch: 9/25... Step: 1120... Loss: 1.4233... Val Loss: 1.5694\n",
      "Epoch: 9/25... Step: 1130... Loss: 1.4386... Val Loss: 1.5618\n",
      "Epoch: 9/25... Step: 1140... Loss: 1.4196... Val Loss: 1.5631\n",
      "Epoch: 9/25... Step: 1150... Loss: 1.4485... Val Loss: 1.5603\n",
      "Epoch: 9/25... Step: 1160... Loss: 1.4018... Val Loss: 1.5580\n",
      "Epoch: 9/25... Step: 1170... Loss: 1.4216... Val Loss: 1.5565\n",
      "Epoch: 9/25... Step: 1180... Loss: 1.3917... Val Loss: 1.5558\n",
      "Epoch: 9/25... Step: 1190... Loss: 1.4368... Val Loss: 1.5524\n",
      "Epoch: 9/25... Step: 1200... Loss: 1.3901... Val Loss: 1.5545\n",
      "Epoch: 9/25... Step: 1210... Loss: 1.3946... Val Loss: 1.5473\n",
      "Epoch: 9/25... Step: 1220... Loss: 1.4029... Val Loss: 1.5469\n",
      "Epoch: 9/25... Step: 1230... Loss: 1.3672... Val Loss: 1.5582\n",
      "Epoch: 9/25... Step: 1240... Loss: 1.3908... Val Loss: 1.5380\n",
      "Epoch: 9/25... Step: 1250... Loss: 1.3984... Val Loss: 1.5446\n",
      "Epoch: 10/25... Step: 1260... Loss: 1.3973... Val Loss: 1.5397\n",
      "Epoch: 10/25... Step: 1270... Loss: 1.3968... Val Loss: 1.5318\n",
      "Epoch: 10/25... Step: 1280... Loss: 1.4043... Val Loss: 1.5333\n",
      "Epoch: 10/25... Step: 1290... Loss: 1.3886... Val Loss: 1.5388\n",
      "Epoch: 10/25... Step: 1300... Loss: 1.3855... Val Loss: 1.5393\n",
      "Epoch: 10/25... Step: 1310... Loss: 1.3875... Val Loss: 1.5318\n",
      "Epoch: 10/25... Step: 1320... Loss: 1.3480... Val Loss: 1.5303\n",
      "Epoch: 10/25... Step: 1330... Loss: 1.3642... Val Loss: 1.5337\n",
      "Epoch: 10/25... Step: 1340... Loss: 1.3546... Val Loss: 1.5248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/25... Step: 1350... Loss: 1.3515... Val Loss: 1.5393\n",
      "Epoch: 10/25... Step: 1360... Loss: 1.3561... Val Loss: 1.5256\n",
      "Epoch: 10/25... Step: 1370... Loss: 1.3311... Val Loss: 1.5226\n",
      "Epoch: 10/25... Step: 1380... Loss: 1.3822... Val Loss: 1.5262\n",
      "Epoch: 10/25... Step: 1390... Loss: 1.3948... Val Loss: 1.5202\n",
      "Epoch: 11/25... Step: 1400... Loss: 1.3886... Val Loss: 1.5190\n",
      "Epoch: 11/25... Step: 1410... Loss: 1.3994... Val Loss: 1.5145\n",
      "Epoch: 11/25... Step: 1420... Loss: 1.3873... Val Loss: 1.5082\n",
      "Epoch: 11/25... Step: 1430... Loss: 1.3602... Val Loss: 1.5131\n",
      "Epoch: 11/25... Step: 1440... Loss: 1.3768... Val Loss: 1.5082\n",
      "Epoch: 11/25... Step: 1450... Loss: 1.3090... Val Loss: 1.5069\n",
      "Epoch: 11/25... Step: 1460... Loss: 1.3389... Val Loss: 1.5066\n",
      "Epoch: 11/25... Step: 1470... Loss: 1.3191... Val Loss: 1.5105\n",
      "Epoch: 11/25... Step: 1480... Loss: 1.3547... Val Loss: 1.5066\n",
      "Epoch: 11/25... Step: 1490... Loss: 1.3316... Val Loss: 1.5027\n",
      "Epoch: 11/25... Step: 1500... Loss: 1.3245... Val Loss: 1.5044\n",
      "Epoch: 11/25... Step: 1510... Loss: 1.3075... Val Loss: 1.4981\n",
      "Epoch: 11/25... Step: 1520... Loss: 1.3466... Val Loss: 1.5063\n",
      "Epoch: 12/25... Step: 1530... Loss: 1.3947... Val Loss: 1.5057\n",
      "Epoch: 12/25... Step: 1540... Loss: 1.3526... Val Loss: 1.5044\n",
      "Epoch: 12/25... Step: 1550... Loss: 1.3612... Val Loss: 1.5020\n",
      "Epoch: 12/25... Step: 1560... Loss: 1.3567... Val Loss: 1.4892\n",
      "Epoch: 12/25... Step: 1570... Loss: 1.3192... Val Loss: 1.4945\n",
      "Epoch: 12/25... Step: 1580... Loss: 1.2969... Val Loss: 1.4857\n",
      "Epoch: 12/25... Step: 1590... Loss: 1.2852... Val Loss: 1.4833\n",
      "Epoch: 12/25... Step: 1600... Loss: 1.3172... Val Loss: 1.4857\n",
      "Epoch: 12/25... Step: 1610... Loss: 1.3117... Val Loss: 1.4968\n",
      "Epoch: 12/25... Step: 1620... Loss: 1.3161... Val Loss: 1.4885\n",
      "Epoch: 12/25... Step: 1630... Loss: 1.3396... Val Loss: 1.4863\n",
      "Epoch: 12/25... Step: 1640... Loss: 1.3020... Val Loss: 1.4950\n",
      "Epoch: 12/25... Step: 1650... Loss: 1.2938... Val Loss: 1.4883\n",
      "Epoch: 12/25... Step: 1660... Loss: 1.3460... Val Loss: 1.4951\n",
      "Epoch: 13/25... Step: 1670... Loss: 1.3141... Val Loss: 1.4956\n",
      "Epoch: 13/25... Step: 1680... Loss: 1.3248... Val Loss: 1.4837\n",
      "Epoch: 13/25... Step: 1690... Loss: 1.2943... Val Loss: 1.4799\n",
      "Epoch: 13/25... Step: 1700... Loss: 1.2929... Val Loss: 1.4793\n",
      "Epoch: 13/25... Step: 1710... Loss: 1.2747... Val Loss: 1.4777\n",
      "Epoch: 13/25... Step: 1720... Loss: 1.2903... Val Loss: 1.4712\n",
      "Epoch: 13/25... Step: 1730... Loss: 1.3243... Val Loss: 1.4747\n",
      "Epoch: 13/25... Step: 1740... Loss: 1.3047... Val Loss: 1.4714\n",
      "Epoch: 13/25... Step: 1750... Loss: 1.2603... Val Loss: 1.4762\n",
      "Epoch: 13/25... Step: 1760... Loss: 1.2846... Val Loss: 1.4685\n",
      "Epoch: 13/25... Step: 1770... Loss: 1.3129... Val Loss: 1.4638\n",
      "Epoch: 13/25... Step: 1780... Loss: 1.2865... Val Loss: 1.4760\n",
      "Epoch: 13/25... Step: 1790... Loss: 1.2642... Val Loss: 1.4682\n",
      "Epoch: 13/25... Step: 1800... Loss: 1.3002... Val Loss: 1.4738\n",
      "Epoch: 14/25... Step: 1810... Loss: 1.2935... Val Loss: 1.4649\n",
      "Epoch: 14/25... Step: 1820... Loss: 1.2810... Val Loss: 1.4716\n",
      "Epoch: 14/25... Step: 1830... Loss: 1.3010... Val Loss: 1.4706\n",
      "Epoch: 14/25... Step: 1840... Loss: 1.2419... Val Loss: 1.4560\n",
      "Epoch: 14/25... Step: 1850... Loss: 1.2431... Val Loss: 1.4661\n",
      "Epoch: 14/25... Step: 1860... Loss: 1.2876... Val Loss: 1.4622\n",
      "Epoch: 14/25... Step: 1870... Loss: 1.3070... Val Loss: 1.4567\n",
      "Epoch: 14/25... Step: 1880... Loss: 1.2855... Val Loss: 1.4551\n",
      "Epoch: 14/25... Step: 1890... Loss: 1.3181... Val Loss: 1.4666\n",
      "Epoch: 14/25... Step: 1900... Loss: 1.2907... Val Loss: 1.4633\n",
      "Epoch: 14/25... Step: 1910... Loss: 1.2885... Val Loss: 1.4695\n",
      "Epoch: 14/25... Step: 1920... Loss: 1.2817... Val Loss: 1.4641\n",
      "Epoch: 14/25... Step: 1930... Loss: 1.2498... Val Loss: 1.4607\n",
      "Epoch: 14/25... Step: 1940... Loss: 1.3052... Val Loss: 1.4521\n",
      "Epoch: 15/25... Step: 1950... Loss: 1.2786... Val Loss: 1.4548\n",
      "Epoch: 15/25... Step: 1960... Loss: 1.2765... Val Loss: 1.4611\n",
      "Epoch: 15/25... Step: 1970... Loss: 1.2656... Val Loss: 1.4521\n",
      "Epoch: 15/25... Step: 1980... Loss: 1.2616... Val Loss: 1.4548\n",
      "Epoch: 15/25... Step: 1990... Loss: 1.2569... Val Loss: 1.4679\n",
      "Epoch: 15/25... Step: 2000... Loss: 1.2483... Val Loss: 1.4535\n",
      "Epoch: 15/25... Step: 2010... Loss: 1.2848... Val Loss: 1.4487\n",
      "Epoch: 15/25... Step: 2020... Loss: 1.2827... Val Loss: 1.4533\n",
      "Epoch: 15/25... Step: 2030... Loss: 1.2543... Val Loss: 1.4478\n",
      "Epoch: 15/25... Step: 2040... Loss: 1.2665... Val Loss: 1.4456\n",
      "Epoch: 15/25... Step: 2050... Loss: 1.2500... Val Loss: 1.4536\n",
      "Epoch: 15/25... Step: 2060... Loss: 1.2635... Val Loss: 1.4568\n",
      "Epoch: 15/25... Step: 2070... Loss: 1.2602... Val Loss: 1.4511\n",
      "Epoch: 15/25... Step: 2080... Loss: 1.2629... Val Loss: 1.4576\n",
      "Epoch: 16/25... Step: 2090... Loss: 1.2715... Val Loss: 1.4495\n",
      "Epoch: 16/25... Step: 2100... Loss: 1.2524... Val Loss: 1.4532\n",
      "Epoch: 16/25... Step: 2110... Loss: 1.2429... Val Loss: 1.4470\n",
      "Epoch: 16/25... Step: 2120... Loss: 1.2570... Val Loss: 1.4416\n",
      "Epoch: 16/25... Step: 2130... Loss: 1.2237... Val Loss: 1.4423\n",
      "Epoch: 16/25... Step: 2140... Loss: 1.2425... Val Loss: 1.4378\n",
      "Epoch: 16/25... Step: 2150... Loss: 1.2562... Val Loss: 1.4425\n",
      "Epoch: 16/25... Step: 2160... Loss: 1.2453... Val Loss: 1.4404\n",
      "Epoch: 16/25... Step: 2170... Loss: 1.2480... Val Loss: 1.4382\n",
      "Epoch: 16/25... Step: 2180... Loss: 1.2442... Val Loss: 1.4432\n",
      "Epoch: 16/25... Step: 2190... Loss: 1.2513... Val Loss: 1.4445\n",
      "Epoch: 16/25... Step: 2200... Loss: 1.2425... Val Loss: 1.4499\n",
      "Epoch: 16/25... Step: 2210... Loss: 1.2048... Val Loss: 1.4395\n",
      "Epoch: 16/25... Step: 2220... Loss: 1.2542... Val Loss: 1.4347\n",
      "Epoch: 17/25... Step: 2230... Loss: 1.2167... Val Loss: 1.4370\n",
      "Epoch: 17/25... Step: 2240... Loss: 1.2241... Val Loss: 1.4394\n",
      "Epoch: 17/25... Step: 2250... Loss: 1.2111... Val Loss: 1.4419\n",
      "Epoch: 17/25... Step: 2260... Loss: 1.2316... Val Loss: 1.4394\n",
      "Epoch: 17/25... Step: 2270... Loss: 1.2387... Val Loss: 1.4298\n",
      "Epoch: 17/25... Step: 2280... Loss: 1.2393... Val Loss: 1.4326\n",
      "Epoch: 17/25... Step: 2290... Loss: 1.2401... Val Loss: 1.4323\n",
      "Epoch: 17/25... Step: 2300... Loss: 1.2032... Val Loss: 1.4337\n",
      "Epoch: 17/25... Step: 2310... Loss: 1.2229... Val Loss: 1.4309\n",
      "Epoch: 17/25... Step: 2320... Loss: 1.2234... Val Loss: 1.4357\n",
      "Epoch: 17/25... Step: 2330... Loss: 1.2220... Val Loss: 1.4382\n",
      "Epoch: 17/25... Step: 2340... Loss: 1.2443... Val Loss: 1.4375\n",
      "Epoch: 17/25... Step: 2350... Loss: 1.2416... Val Loss: 1.4287\n",
      "Epoch: 17/25... Step: 2360... Loss: 1.2477... Val Loss: 1.4255\n",
      "Epoch: 18/25... Step: 2370... Loss: 1.2096... Val Loss: 1.4339\n",
      "Epoch: 18/25... Step: 2380... Loss: 1.2232... Val Loss: 1.4254\n",
      "Epoch: 18/25... Step: 2390... Loss: 1.2248... Val Loss: 1.4325\n",
      "Epoch: 18/25... Step: 2400... Loss: 1.2389... Val Loss: 1.4345\n",
      "Epoch: 18/25... Step: 2410... Loss: 1.2422... Val Loss: 1.4313\n",
      "Epoch: 18/25... Step: 2420... Loss: 1.2209... Val Loss: 1.4210\n",
      "Epoch: 18/25... Step: 2430... Loss: 1.2253... Val Loss: 1.4197\n",
      "Epoch: 18/25... Step: 2440... Loss: 1.2091... Val Loss: 1.4311\n",
      "Epoch: 18/25... Step: 2450... Loss: 1.2054... Val Loss: 1.4232\n",
      "Epoch: 18/25... Step: 2460... Loss: 1.2199... Val Loss: 1.4160\n",
      "Epoch: 18/25... Step: 2470... Loss: 1.2160... Val Loss: 1.4256\n",
      "Epoch: 18/25... Step: 2480... Loss: 1.1990... Val Loss: 1.4206\n",
      "Epoch: 18/25... Step: 2490... Loss: 1.1960... Val Loss: 1.4186\n",
      "Epoch: 18/25... Step: 2500... Loss: 1.1937... Val Loss: 1.4232\n",
      "Epoch: 19/25... Step: 2510... Loss: 1.2100... Val Loss: 1.4265\n",
      "Epoch: 19/25... Step: 2520... Loss: 1.2267... Val Loss: 1.4203\n",
      "Epoch: 19/25... Step: 2530... Loss: 1.2228... Val Loss: 1.4134\n",
      "Epoch: 19/25... Step: 2540... Loss: 1.2377... Val Loss: 1.4203\n",
      "Epoch: 19/25... Step: 2550... Loss: 1.1925... Val Loss: 1.4237\n",
      "Epoch: 19/25... Step: 2560... Loss: 1.2130... Val Loss: 1.4176\n",
      "Epoch: 19/25... Step: 2570... Loss: 1.1969... Val Loss: 1.4115\n",
      "Epoch: 19/25... Step: 2580... Loss: 1.2285... Val Loss: 1.4238\n",
      "Epoch: 19/25... Step: 2590... Loss: 1.1978... Val Loss: 1.4165\n",
      "Epoch: 19/25... Step: 2600... Loss: 1.2002... Val Loss: 1.4177\n",
      "Epoch: 19/25... Step: 2610... Loss: 1.2160... Val Loss: 1.4194\n",
      "Epoch: 19/25... Step: 2620... Loss: 1.1790... Val Loss: 1.4145\n",
      "Epoch: 19/25... Step: 2630... Loss: 1.1877... Val Loss: 1.4101\n",
      "Epoch: 19/25... Step: 2640... Loss: 1.2047... Val Loss: 1.4109\n",
      "Epoch: 20/25... Step: 2650... Loss: 1.2076... Val Loss: 1.4215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/25... Step: 2660... Loss: 1.2001... Val Loss: 1.4126\n",
      "Epoch: 20/25... Step: 2670... Loss: 1.2124... Val Loss: 1.4116\n",
      "Epoch: 20/25... Step: 2680... Loss: 1.2012... Val Loss: 1.4131\n",
      "Epoch: 20/25... Step: 2690... Loss: 1.1944... Val Loss: 1.4157\n",
      "Epoch: 20/25... Step: 2700... Loss: 1.2080... Val Loss: 1.4156\n",
      "Epoch: 20/25... Step: 2710... Loss: 1.1750... Val Loss: 1.4123\n",
      "Epoch: 20/25... Step: 2720... Loss: 1.1748... Val Loss: 1.4112\n",
      "Epoch: 20/25... Step: 2730... Loss: 1.1735... Val Loss: 1.4056\n",
      "Epoch: 20/25... Step: 2740... Loss: 1.1677... Val Loss: 1.4082\n",
      "Epoch: 20/25... Step: 2750... Loss: 1.1804... Val Loss: 1.4091\n",
      "Epoch: 20/25... Step: 2760... Loss: 1.1630... Val Loss: 1.4063\n",
      "Epoch: 20/25... Step: 2770... Loss: 1.2109... Val Loss: 1.4031\n",
      "Epoch: 20/25... Step: 2780... Loss: 1.2316... Val Loss: 1.4089\n",
      "Epoch: 21/25... Step: 2790... Loss: 1.2116... Val Loss: 1.4105\n",
      "Epoch: 21/25... Step: 2800... Loss: 1.2248... Val Loss: 1.4031\n",
      "Epoch: 21/25... Step: 2810... Loss: 1.2257... Val Loss: 1.3970\n",
      "Epoch: 21/25... Step: 2820... Loss: 1.1856... Val Loss: 1.4038\n",
      "Epoch: 21/25... Step: 2830... Loss: 1.1903... Val Loss: 1.4113\n",
      "Epoch: 21/25... Step: 2840... Loss: 1.1443... Val Loss: 1.4070\n",
      "Epoch: 21/25... Step: 2850... Loss: 1.1722... Val Loss: 1.4036\n",
      "Epoch: 21/25... Step: 2860... Loss: 1.1519... Val Loss: 1.4096\n",
      "Epoch: 21/25... Step: 2870... Loss: 1.1779... Val Loss: 1.4102\n",
      "Epoch: 21/25... Step: 2880... Loss: 1.1685... Val Loss: 1.4142\n",
      "Epoch: 21/25... Step: 2890... Loss: 1.1609... Val Loss: 1.4021\n",
      "Epoch: 21/25... Step: 2900... Loss: 1.1567... Val Loss: 1.4047\n",
      "Epoch: 21/25... Step: 2910... Loss: 1.1864... Val Loss: 1.4044\n",
      "Epoch: 22/25... Step: 2920... Loss: 1.2824... Val Loss: 1.4042\n",
      "Epoch: 22/25... Step: 2930... Loss: 1.1955... Val Loss: 1.4040\n",
      "Epoch: 22/25... Step: 2940... Loss: 1.1961... Val Loss: 1.3933\n",
      "Epoch: 22/25... Step: 2950... Loss: 1.2035... Val Loss: 1.4002\n",
      "Epoch: 22/25... Step: 2960... Loss: 1.1562... Val Loss: 1.4035\n",
      "Epoch: 22/25... Step: 2970... Loss: 1.1454... Val Loss: 1.4139\n",
      "Epoch: 22/25... Step: 2980... Loss: 1.1472... Val Loss: 1.4181\n",
      "Epoch: 22/25... Step: 2990... Loss: 1.1702... Val Loss: 1.4014\n",
      "Epoch: 22/25... Step: 3000... Loss: 1.1537... Val Loss: 1.4062\n",
      "Epoch: 22/25... Step: 3010... Loss: 1.1508... Val Loss: 1.3959\n",
      "Epoch: 22/25... Step: 3020... Loss: 1.1682... Val Loss: 1.4075\n",
      "Epoch: 22/25... Step: 3030... Loss: 1.1556... Val Loss: 1.4063\n",
      "Epoch: 22/25... Step: 3040... Loss: 1.1479... Val Loss: 1.4043\n",
      "Epoch: 22/25... Step: 3050... Loss: 1.1964... Val Loss: 1.4022\n",
      "Epoch: 23/25... Step: 3060... Loss: 1.1647... Val Loss: 1.3964\n",
      "Epoch: 23/25... Step: 3070... Loss: 1.1657... Val Loss: 1.4037\n",
      "Epoch: 23/25... Step: 3080... Loss: 1.1497... Val Loss: 1.3986\n",
      "Epoch: 23/25... Step: 3090... Loss: 1.1546... Val Loss: 1.3984\n",
      "Epoch: 23/25... Step: 3100... Loss: 1.1456... Val Loss: 1.3984\n",
      "Epoch: 23/25... Step: 3110... Loss: 1.1435... Val Loss: 1.3984\n",
      "Epoch: 23/25... Step: 3120... Loss: 1.1746... Val Loss: 1.4035\n",
      "Epoch: 23/25... Step: 3130... Loss: 1.1507... Val Loss: 1.3906\n",
      "Epoch: 23/25... Step: 3140... Loss: 1.1215... Val Loss: 1.3997\n",
      "Epoch: 23/25... Step: 3150... Loss: 1.1451... Val Loss: 1.4020\n",
      "Epoch: 23/25... Step: 3160... Loss: 1.1664... Val Loss: 1.4037\n",
      "Epoch: 23/25... Step: 3170... Loss: 1.1500... Val Loss: 1.4024\n",
      "Epoch: 23/25... Step: 3180... Loss: 1.1299... Val Loss: 1.3992\n",
      "Epoch: 23/25... Step: 3190... Loss: 1.1578... Val Loss: 1.3959\n",
      "Epoch: 24/25... Step: 3200... Loss: 1.1698... Val Loss: 1.3913\n",
      "Epoch: 24/25... Step: 3210... Loss: 1.1438... Val Loss: 1.4046\n",
      "Epoch: 24/25... Step: 3220... Loss: 1.1719... Val Loss: 1.3881\n",
      "Epoch: 24/25... Step: 3230... Loss: 1.1145... Val Loss: 1.3941\n",
      "Epoch: 24/25... Step: 3240... Loss: 1.1077... Val Loss: 1.3961\n",
      "Epoch: 24/25... Step: 3250... Loss: 1.1617... Val Loss: 1.3983\n",
      "Epoch: 24/25... Step: 3260... Loss: 1.1647... Val Loss: 1.3984\n",
      "Epoch: 24/25... Step: 3270... Loss: 1.1597... Val Loss: 1.3964\n",
      "Epoch: 24/25... Step: 3280... Loss: 1.1778... Val Loss: 1.4030\n",
      "Epoch: 24/25... Step: 3290... Loss: 1.1542... Val Loss: 1.3997\n",
      "Epoch: 24/25... Step: 3300... Loss: 1.1462... Val Loss: 1.3995\n",
      "Epoch: 24/25... Step: 3310... Loss: 1.1426... Val Loss: 1.3975\n",
      "Epoch: 24/25... Step: 3320... Loss: 1.1181... Val Loss: 1.3922\n",
      "Epoch: 24/25... Step: 3330... Loss: 1.1655... Val Loss: 1.3873\n",
      "Epoch: 25/25... Step: 3340... Loss: 1.1353... Val Loss: 1.3854\n",
      "Epoch: 25/25... Step: 3350... Loss: 1.1472... Val Loss: 1.3961\n",
      "Epoch: 25/25... Step: 3360... Loss: 1.1326... Val Loss: 1.3900\n",
      "Epoch: 25/25... Step: 3370... Loss: 1.1347... Val Loss: 1.4015\n",
      "Epoch: 25/25... Step: 3380... Loss: 1.1336... Val Loss: 1.3983\n",
      "Epoch: 25/25... Step: 3390... Loss: 1.1269... Val Loss: 1.4018\n",
      "Epoch: 25/25... Step: 3400... Loss: 1.1400... Val Loss: 1.4009\n",
      "Epoch: 25/25... Step: 3410... Loss: 1.1529... Val Loss: 1.3965\n",
      "Epoch: 25/25... Step: 3420... Loss: 1.1254... Val Loss: 1.3931\n",
      "Epoch: 25/25... Step: 3430... Loss: 1.1457... Val Loss: 1.3956\n",
      "Epoch: 25/25... Step: 3440... Loss: 1.1313... Val Loss: 1.4016\n",
      "Epoch: 25/25... Step: 3450... Loss: 1.1301... Val Loss: 1.4038\n",
      "Epoch: 25/25... Step: 3460... Loss: 1.1469... Val Loss: 1.3884\n",
      "Epoch: 25/25... Step: 3470... Loss: 1.1417... Val Loss: 1.3888\n"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "\n",
    "train(net, encoded, epochs=25, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8439d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_25_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2747d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    \n",
    "    h = net.init_hidden(1)\n",
    "    \n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        \n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "003c18dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna had been\n",
      "thereful, the promase seemed to ask her heart that had been\n",
      "asked him about it. At the picture of her simple tries they were\n",
      "there the bits of the counters of a minute from the story of\n",
      "the carriage in the server to her heaved in a found of a sport merchance,\n",
      "her lover, a condingers held him the prayers of her, and almost in the\n",
      "solutions of the stands, the music worse to the carriage, he would hear\n",
      "them, and she came into the carryarm.\n",
      "\n",
      "\"Why, you'll be thinking about this face, I should have been talking of him to\n",
      "her.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 22\n",
      "\n",
      "\n",
      "\"Yes, I've not been a cruel of the soul of the pordiculation of\n",
      "this whole fright of the children and he wanted to talk about it.\" He\n",
      "took the same as a smile of consciousness of an impossibility in\n",
      "the country house as he was the carriage of the desire in the priest of\n",
      "the children, and so sort of having shutt the corridor.\n",
      "\n",
      "\"I say anything,\" she said, and said her eyes asked it was not his\n",
      "soft hair, whatever in the following on his big horse, a too, and\n",
      "husband.\n",
      "\n",
      "As he had stricked him for the sound, he had not to belong. All the\n",
      "more\n",
      "propected in his heart the stricken hands out of his hands. And the\n",
      "doctor always said it. As she was silent, and they had been all over the simplete\n",
      "closs of his humiliation, while, she was driving on the same thing who\n",
      "had had the second table, to herself all about her he could be so into\n",
      "her eyes, and though they had at a collor of the dishonest to the priest,\n",
      "and he felt indown, turned a portfolio of this mud terrible of\n",
      "the peculiar boy, he found the people still was as it to tears, the prasens\n",
      "had begged it only a mass of the whole, and that was her his from\n",
      "work he was than some other and the fact, he found the same\n",
      "dinner to the man with themselves as he went, because she was the\n",
      "fould he struggled.\n",
      "\n",
      "\"A doon's so instincted you shall be the frish of servent wearings.\n",
      "\n",
      "I want to dream on the few attachmens, and wonderfully doubt, and the\n",
      "doctor answered why, I'm no day with him. The m\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 2000, prime='Anna', top_k=5, cuda=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65c741d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over 1 epoch `rnn_1_epoch.net`\n",
    "with open('rnn_25_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7970180d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said he\n",
      "could not help seeing her son, as a point of all, staying his stairs,\n",
      "and trapsicaling in the hand, and she found the fact.\n",
      "\n",
      "\n",
      "\n",
      "Chapter 7\n",
      "\n",
      "\n",
      "The captain was of a smaller old coach treating them, but her\n",
      "shoulder and words and the characteristic of the strong, taken tase\n",
      "at the strupt to her, than that without all. But that showed this child to her\n",
      "heart so impossible to think of the sorries, the sight of the partion, and\n",
      "who could not be too, he was any more assisted on that this simple the\n",
      "meadows of that happiness of which there was now his below in a child.\n",
      "Stepan Arkadyevitch had tears with went along with a smule of\n",
      "heart well as he tried to be at the four too him that she\n",
      "sent, to the day to this party the crimining on his sight of the\n",
      "consciousness. Here suffering which was some too which had been strained on his\n",
      "husband and wound them with her story of the same sense of her.\n",
      "\n",
      "\"I should be it was able to go into the chief some of men and that to\n",
      "dispute, and were answered while it was now the same thing. It was\n",
      "so seconding that that he did not go to him that he must have the\n",
      "better about these servant. And they want, befror hunt, and wanted to\n",
      "stand at the pirture, I am not going towards it.\" Sergey Ivanovitch was asked.\n",
      "\n",
      "\"Well, the carriage to be mainting a much from him is this fatchet of me\n",
      "alone, and the conditions and myself, and seen hards on the fact as an old,\n",
      "and how the most importances with her succicined as her arm, but\n",
      "she was so time to do what to come. But the papar of which had been a\n",
      "stony towards with his face to a set of attractively.\n",
      "\n",
      "\"It would be that to me.\"\n",
      "\n",
      "Stepan Arkadyevitch was a familiar bound to the strings, the country\n",
      "and he did not speak out of anything. And he saw it all the princess of his\n",
      "house and they says the man with haste over the meatings. The self-canse\n",
      "of her husband, and the prince waited to take the club to a promise and\n",
      "happiness, but though her face, a landowner, who had already decoved\n",
      "that it was singer a sti\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 2000, cuda=True, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a2ba215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for which he would\n",
      "be in a countess and without the clerk took a conscious of the meetings, and\n",
      "through the study of the wede the coats and heart of sense of\n",
      "herself, was a little sorry and with all the simples\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 200, cuda=True, top_k=5, prime=\"for which\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f350f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've a particular hise\n",
      "at that countess, and were at\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 50, cuda=True, top_k=5, prime=\"I\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb7ca36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
